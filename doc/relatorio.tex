\documentclass[12pt]{article}
\usepackage[a4paper,margin=1in,footskip=0.25in]{geometry} % set margins
\usepackage[portuguese]{babel}
\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref} 
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}    % needed for include graphics
\usepackage{graphics}
\usepackage{subfigure}   % add subfigures
\usepackage{indentfirst}
\usepackage{float}       % needed for [H] figure placement option
\usepackage{setspace}    % needed for doublespacing
\usepackage{tikz}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{CStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

% Macros
\renewcommand{\familydefault}{\sfdefault} % sans-serif
\newcommand{\lowtext}[1]{$_{\text{#1}}$}
\newcommand{\code}[1]{\texttt{#1}}

% Adds ./img/ to the path of figures
\graphicspath{{./img/}}

\title{Relatório EP3 - MAC0219}
\author{Bruno Sesso, Gustavo Estrela de Matos, Lucas Sung Jun Hong}

\begin{document}
% Espaçamento duplo 
\doublespacing
\begin{titlepage}
    \vfill
    \begin{center}
        \vspace{0.5\textheight}
        \noindent
        Instituto de Matemática e Estatística \\
        EP3 - MAC0219 \\
        \vfill
        \noindent
        {\Large Cálculo do Conjunto de Mandelbrot
        em Paralelo com OpenMPI} \\
        \bigskip
        \bigskip
        \begin{tabular}{ll}
            {\bf Professor:} & {Alfredo Goldman} \\
            {\bf Alunos:}    & {Bruno Sesso} \\
                             & {Gustavo Estrela de Matos} \\
        \end{tabular} \\
        \vspace{\fill}
       \bigskip
        São Paulo, \today \\
       \bigskip
    \end{center}
\end{titlepage}

\pagebreak
\tableofcontents
\pagebreak

\section{Introdução}
Neste trabalho, implementaremos uma versão paralela, em OpenMPI, do 
código que calcula o fractal de uma região de Mandelbrot. Esta versão
deve ser capaz de, dado um cluster de computadores, utilizar o padrão
de troca de mensagens MPI para distribuir e reunir em um nó principal
os cálculos necessários para gerar a imagem da região pedida.

Ao longo desse trabalho, iremos apresentar resultados de tempo de 
execução das diferentes implementações e suas respectivas variações.
Para isso, utilizamos a ferramenta {\em perf}, capaz de realizar 
repetições de experimentos, apresentando resultados médios e com desvio
padrão. Todos os resultados apresentados aqui foram feitos a partir de
no mínimo 10 execuções do mesmo comando.
        

%%%%%%%%%%%%%%%%%%%%% SEQUENCIAL %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Código Sequencial}
O código sequencial foi fornecido no enunciado do EP e com ele fizemos
medições de tempo para diferentes regiões do plano complexo.

\begin{figure}[H]
    \makebox[\textwidth][c]{\includegraphics[scale=.50]{seq_with/all_timeXsize_OpenMPpng.png}}
    \caption{Tempo de execução do programa sequencial para diferentes 
    tamanhos de entrada}
\end{figure}

Nota-se que o tempo em cada região é diferente, pois em cada região
pode haver mais cálculos de pontos com mais interações. Em especial, 
podemos observar que a região Full é a mais fácil de ser calculada.

Como explicamos no EP1, o cálculo dessas regiões escala de uma maneira,
aparentemente, mais do que linear. Além disso, o consumo de tempo 
causado por operações de alocação, leitura e escrita de memória são 
lineares e serão desconsideradas nesse trabalho.



%%%%%%%%%%%%%%%%%%%%% OPENMPI %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Código em OpenMPI}
A nossa implementação em OpenMPI define uma máquina como principal 
(mestre), e esta deve ser capaz de dividir o trabalho entre as outras 
máquinas (escrava), assim como reunir os resultados para construir a 
imagem da região Mandelbrot. Para fazer isso, dividimos os pontos da 
imagem em blocos de pixels e, então a máquina principal envia para 
outras máquinas os blocos que devem ser calculados, por meio de uma 
mensagem que indica o índice do bloco que deve ser calculado. Depois de
calcular o bloco, a escrava envia para mestre um vetor que contém as
iterações calculadas naquele bloco para compor a imagem.

A estrutura de blocos que usamos neste trabalho é a mesma usada no EP1 
e, como fizemos na última vez, vamos dividir a imagem em $n$ blocos, em
que $n$ é o tamanho do lado da imagem, isto é, cada bloco corresponde a
uma linha da imagem. Além disso, como constatamos no EP1 que uma divisão
dinâmica de trabalho é mais vantajosa, vamos adotar essa abordagem 
também neste trabalho. Ao invés de pré-determinar em qual máquina um
bloco será calculado, delegamos a máquinas livres blocos que ainda não
foram calculados.

Para implementar essa divisão dinâmica de trabalho, a máquina mestre 
deve iniciar o processo delegando um bloco para cada máquina escrava.
Depois, sempre que uma máquina enviar o trabalho pronto, a mestre deve
processar essa resposta, atualizando o buffer da imagem, e enviar a
escrava um novo bloco para ser calculado (se houver). Para finalizar o
processo, a mestre também deve enviar um sinal de "trabalho feito" para
todas escravas, o que faz possível terminar o programa em todas as 
máquinas do cluster.

Veja abaixo um pseudo-código da função que roda na máquina mestre, 
distribuindo os blocos de pixels para as máquinas escravas:
\begin{algorithmic}[1]
\Function{DistributeWork}{}
    \State $S \gets $ lista de nacos de todos os pixels
    \State $sent\_chunks \gets 0$
    \State $received\_chunks \gets 0$ 
    \ForAll{$M$ máquina escrava do cluster}
        \State $Send (M, S[sent\_chunks])$
        \State $sent\_chunks \gets sent\_chunks + 1$
    \EndFor
    \While{$received\_chunks < |S|$}
        \State $N \gets PullJobFromAnyMachine ()$
        \Comment{Recebe trabalho de escrava N}
        \State $received\_chunks \gets received\_chunks + 1$
        \If{$sent\_chunks < |S|$}
            \State $Send (N, S[sent\_chunks])$
            \State $sent\_chunks \gets sent\_chunks + 1$
        \Else
            \State $Send (N, END\_CODE)$
        \EndIf
    \EndWhile
\EndFunction
\end{algorithmic}

\subsection{Experimentos}
Para verificar como o nosso algoritmo se comporta, decidimos rodar 
instâncias do problema para diferentes situações de processamento 
paralelo usando OpenMPI. Mantemos o número total de cores em 8, variando
o número de máquinas de 1 até 8. Medimos o tempo de execução para 
instâncias de tamanho 11500x11500 nas regiões Full, Elephant, Seahorse e
Spiral.

\begin{table}[ht] 
    \begin{center} 
    \begin{tabular}{@{}lc cc@{}} 
        \multicolumn{4}{c}{Tempo de execução médio (segundos)}\\
        \hline
        \hline
        Experimento && Full & Elephant \\
        \hline
        1 instância com 8 cores &&  
                                    23,49 ($\pm$ 0,60\%) &  
                                    45,69 ($\pm$ 0,30\%) \\ 
        2 instâncias com 4 cores && 
                                    23,97 ($\pm$ 0,28\%) &
                                    60,27 ($\pm$ 0,17\%) \\
        4 instâncias com 2 cores && 
                                    33,99 ($\pm$ 0,52\%) &
                                    91,67 ($\pm$ 0,14\%) \\
        8 instâncias com 1 core && 
                                   141,48 ($\pm$ 0,09\%) &
                                   203,58 ($\pm$ 0,91\%) \\

        \hline
        Experimento && Seahorse & Tripple  Spiral \\
        \hline
        1 instância com 8 cores &&
                                    47,20 ($\pm$ 0.18\%) & 
                                    51,64 ($\pm$ 0,24\%) \\
        2 instâncias com 4 cores && 
                                    62,71 ($\pm$ 0,13\%) &
                                    69,02 ($\pm$ 0,14\%) \\
        4 instâncias com 2 cores && 
                                    92,70 ($\pm$ 1,19\%) &
                                    100,58 ($\pm$ 0,14\%) \\
        8 instâncias com 1 core && 
                                   176,16 ($\pm$ 0,08\%) &
                                   218,60 ($\pm$ 0,10\%) \\
        \hline
        \hline
    \end{tabular} \caption{Resultados de tempo de execução para 
        implementação em OpenMPI pra imagens de $11500^2$ pixels.}
        \label{tabela:openmpi}
    \end{center} 
\end{table}

Como podemos observar na tabela \ref{tabela:openmpi} quanto mais 
máquinas usamos, maior é o tempo de execução de nosso programa. Isso 
acontece porque quando usamos mais máquinas precisamos fazer 
comunicações pela rede durante a execução do programa, o que é mais 
lento do que acessar diretamente a memória.

Um cuidado que não tomamos durante nossa implementação, e que poderia
resultar em um desempenho melhor mesmo com mais máquinas, é evitar fazer
trocas de mensagens no código. Nossa implementação imita a melhor 
implementação de pthreads feita no EP1, onde utilizamos uma divisão
dinâmica de trabalho, porém, esse tipo de implementação em OpenMPI 
implica em um número maior de troca de mensagens quando comparado a uma
implementação com divisão de trabalho estática.

\newpage
\section{Conclusão}

\newpage
\end{document}
